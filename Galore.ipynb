{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#GPT2 from the karpathy video\n",
        "#Train it with regular (from scratch adam)\n",
        "#Also train it with Galore\n",
        "\n"
      ],
      "metadata": {
        "id": "gv7uNqchOwCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karpathy GPT2 Model"
      ],
      "metadata": {
        "id": "JfdesmS-qij5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0, 'heads dont match'\n",
        "        #Key,query, value projection for all heads but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias = False)\n",
        "        #output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
        "        #regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embd\n",
        "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() #batch size, seq length, embedding dimensionality\n",
        "        #calculate k,q,v for all heads in batch and move head forward to be the batch\n",
        "        #nh is number of heads, hs is head size, c is channgel\n",
        "        #split into heads\n",
        "        qkv = self.c_attn(x)\n",
        "        q,k,v = qkv.split(self.n_embed,dim = 2)\n",
        "        k = k.view(B,T,self.n_head, C // self.n_head).transpose(1,2) #(B, nh, t ,hs)\n",
        "        q = q.view(B,T,self.n_head, C // self.n_head).transpose(1,2) #(B, nh, t ,hs)\n",
        "        v = v.view(B,T,self.n_head, C // self.n_head).transpose(1,2) #(B, nh, t ,hs)\n",
        "        #attention (materliazes the large (T,T) matrix for all q's and v's)\n",
        "        #-------Old Atention------\n",
        "        #att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        #att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) #makes it attend to only past tokens never future\n",
        "        #att = F.softmax(att, dim = -1)\n",
        "        #y = att @ v #(B,nh, T,T) x (B,nh, T ,hs) -> (B,nh, T, hs)\n",
        "        #------------------------\n",
        "        #Flash Attention\n",
        "        y = F.scaled_dot_product_attention(q,k,v,is_causal = True)\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,C) #reassemble all outputs\n",
        "        y = self.c_proj(y) #GPT proposes to scale this down by 1/sqrt(N)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate = 'tanh')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "#Transformers are reduce functions\n",
        "#MLP are map\n",
        "#think of attention as a map reduce\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int = 50304\n",
        "    block_size: int = 1024\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
        "            ln_f = nn.LayerNorm(config.n_embd)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "      #Weight sharing scheme (copies the data pointer)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "      #init params\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self,module):\n",
        "        if isinstance(module,nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module,'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5 #each layer has 2 blocks attention and MLP\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module,nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
        "\n",
        "    def forward(self, idx,targets = None):\n",
        "      #idx is of shape (B,T)\n",
        "      B,T = idx.size()\n",
        "      assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "      pos = torch.arange(0,T,device = idx.device)\n",
        "      pos_emb = self.transformer.wpe(pos)\n",
        "      tok_emb = self.transformer.wte(idx)\n",
        "      x = tok_emb + pos_emb\n",
        "      for block in self.transformer.h:\n",
        "        x = block(x)\n",
        "\n",
        "      x = self.transformer.ln_f(x)\n",
        "      logits = self.lm_head(x) #(B,T, vocab_size)\n",
        "      loss = None\n",
        "      if targets is not None:\n",
        "\n",
        "        loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
        "      return logits,loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls,model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "    def configure_optimizer(self,weight_decay,learning_rate,device):\n",
        "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "      #create optim groups any paratemer that is 2D gets decayed otherwise not\n",
        "      #i.e. all weight tensors in matmuls + embeddings get decayed and layernorms dont\n",
        "      decay_params = [p for n,p in param_dict.items() if p.dim()>= 2]\n",
        "      nodecay_params = [p for n,p in param_dict.items() if p.dim()< 2]\n",
        "      optim_groups = [\n",
        "          {'params':decay_params,'weight_decay':weight_decay},\n",
        "          {'params':nodecay_params,'weight_decay':0.0}\n",
        "      ]\n",
        "      num_decay_params = sum(p.numel() for p in decay_params)\n",
        "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "      print(f'num decay params: {num_decay_params}')\n",
        "      print(f'num nodecay params: {num_nodecay_params}')\n",
        "      #CreateAdamW optimizer\n",
        "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "      #fused means that all kernels are fused into single kernel so a single time on all parameters you update them\n",
        "      use_fused = fused_available and 'cuda' in device\n",
        "      print(f'using fusedAdamW: {use_fused}')\n",
        "      optimizer = torch.optim.AdamW(optim_groups,lr = learning_rate,fused = use_fused,betas = (0.9,0.95),eps = 1e-8)\n",
        "      return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "  def __init__(self,B,T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    # at init load tokens from disk and store them in memory\n",
        "\n",
        "    with open('input.txt', 'r') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f'loaded {len(self.tokens)} tokens')\n",
        "\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B,T = self.B,self.T\n",
        "    buf = self.tokens[self.current_position:self.current_position+B*T+1]\n",
        "    x = (buf[:-1]).view(B,T)\n",
        "    y = (buf[1:]).view(B,T)\n",
        "    self.current_position += B*T\n",
        "\n",
        "    if self.current_position + (B*T + 1)> len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "fuHFKzl1PE4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataSet Retrieval"
      ],
      "metadata": {
        "id": "DvPCHLxgql9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken\n",
        "import tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sreugR7PPL7Q",
        "outputId": "90b1f8a2-9fce-44a3-a34e-04960647199d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-07 19:25:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-07-07 19:25:37 (139 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_iters = 10\n",
        "max_steps = 50\n",
        "def get_lr(step):\n",
        "  #1 Linear warmup for warmup_iters steps\n",
        "  if step < warmup_iters:\n",
        "    return max_lr * (step + 1) / warmup_iters\n",
        "  #2 if step > lr_decay_iters retun min learning rate\n",
        "  if step > max_steps:\n",
        "    return min_lr\n",
        "  #3 in bewtween use cosine decay\n",
        "  decay_ratio = (step - warmup_iters) / (max_steps - warmup_iters)\n",
        "  assert 0<=decay_ratio<=1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "import time\n",
        "model = GPT(GPTConfig())\n",
        "model = torch.compile(model)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "train_loader = DataLoaderLite(16,1024)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om-2BFTuPNPf",
        "outputId": "74edad12-e022-40f8-eef0-cd45b7dbc87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Galore Paremeters"
      ],
      "metadata": {
        "id": "K1V78M3APUMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rank, subspace change frequency\n",
        "rank = 5\n",
        "subspace_change_freq = 50\n",
        "\n",
        "target_modules_list = ['attn','mlp']\n",
        "galore_params = []\n",
        "\n",
        "for module_name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "\n",
        "    if not any(target_key in module_name for target_key in target_modules_list):\n",
        "        continue\n",
        "\n",
        "    print('enable GaLore for weights in module: ', module_name)\n",
        "    galore_params.append(module.weight)\n",
        "\n",
        "id_galore_params = [id(p) for p in galore_params]\n",
        "# make parameters without \"rank\" to another group\n",
        "regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
        "# then call galore_adamw\n",
        "param_groups = [{'params': regular_params},\n",
        "                {'params': galore_params, 'rank': rank, 'subspace_change_freq': subspace_change_freq}]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-_s8n_QPSDB",
        "outputId": "7fe56312-b702-426c-a49c-a59623d0f255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enable GaLore for weights in module:  _orig_mod.transformer.h.0.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.0.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.0.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.0.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.1.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.1.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.1.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.1.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.2.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.2.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.2.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.2.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.3.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.3.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.3.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.3.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.4.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.4.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.4.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.4.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.5.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.5.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.5.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.5.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.6.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.6.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.6.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.6.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.7.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.7.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.7.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.7.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.8.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.8.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.8.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.8.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.9.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.9.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.9.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.9.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.10.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.10.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.10.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.10.mlp.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.11.attn.c_attn\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.11.attn.c_proj\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.11.mlp.c_fc\n",
            "enable GaLore for weights in module:  _orig_mod.transformer.h.11.mlp.c_proj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GaloreProjecter:\n",
        "  def __init__(self,rank,subspace_change_freq):\n",
        "     self.rank = rank\n",
        "     self.subspace_change_freq = subspace_change_freq\n",
        "     self.ortho_matrix = None\n",
        "\n",
        "  def project(self,full_rank_grad, iter ):\n",
        "    if full_rank_grad.shape[0] >= full_rank_grad.shape[1]:\n",
        "      if self.ortho_matrix is None or iter % self.subspace_change_freq == 0:\n",
        "        self.ortho_matrix = self.SVD(full_rank_grad,self.rank,'right')\n",
        "      low_rank_grad = torch.matmul(full_rank_grad,self.ortho_matrix.t())\n",
        "    else:\n",
        "      if self.ortho_matrix is None or iter % self.subspace_change_freq == 0:\n",
        "        self.ortho_matrix = self.SVD(full_rank_grad,self.rank,'left')\n",
        "      low_rank_grad = torch.matmul(self.ortho_matrix.t(),full_rank_grad)\n",
        "\n",
        "    return low_rank_grad\n",
        "\n",
        "  def projectback(self,low_rank_grad):\n",
        "    if low_rank_grad.shape[0] >= low_rank_grad.shape[1]:\n",
        "      full_rank_grad = torch.matmul(low_rank_grad,self.ortho_matrix)\n",
        "    else:\n",
        "      full_rank_grad = torch.matmul(self.ortho_matrix,low_rank_grad)\n",
        "    return full_rank_grad\n",
        "\n",
        "  def SVD(self,weights,rank,type):\n",
        "    module_params = weights\n",
        "\n",
        "    if module_params.data.dtype != torch.float:\n",
        "        float_data = False\n",
        "        original_type = module_params.data.dtype\n",
        "        original_device = module_params.data.device\n",
        "        matrix = module_params.data.float()\n",
        "    else:\n",
        "        float_data = True\n",
        "        matrix = module_params.data\n",
        "\n",
        "    U,S,V = torch.linalg.svd(matrix,full_matrices = False)\n",
        "\n",
        "    if type == 'right':\n",
        "      B = V[:rank,:]\n",
        "      if not float_data:\n",
        "        B = B.to(original_device).type(original_type)\n",
        "      return B\n",
        "    if type == 'left':\n",
        "      A = U[:,:rank]\n",
        "      if not float_data:\n",
        "        A = A.to(original_device).type(original_type)\n",
        "      return A\n",
        "    else:\n",
        "      raise ValueError(f'unknown type {type}')\n",
        "\n"
      ],
      "metadata": {
        "id": "YQpOgjiqQzVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "import math\n",
        "import warnings\n",
        "from typing import Callable, Iterable, Tuple\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[nn.parameter.Parameter],\n",
        "        lr: float = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-6,\n",
        "        weight_decay: float = 0.0,\n",
        "        correct_bias: bool = True,\n",
        "        no_deprecation_warning: bool = False,\n",
        "    ):\n",
        "      if lr < 0.0:\n",
        "              raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
        "      if not 0.0 <= betas[0] < 1.0:\n",
        "          raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
        "      if not 0.0 <= betas[1] < 1.0:\n",
        "          raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
        "      if not 0.0 <= eps:\n",
        "          raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
        "      defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n",
        "      super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self,closure: Callable = None):\n",
        "      \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
        "        \"\"\"\n",
        "      loss = None\n",
        "      if closure is not None:\n",
        "          loss = closure()\n",
        "\n",
        "      for group in self.param_groups:\n",
        "        for p in group['params']:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad\n",
        "            if grad.is_sparse:\n",
        "              raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
        "\n",
        "            state = self.state[p]\n",
        "\n",
        "            if 'step' not in state:\n",
        "                state['step'] = 0\n",
        "\n",
        "            #intialize the state\n",
        "            if 'exp_avg' not in state:\n",
        "                state['exp_avg'] = torch.zeros_like(grad) #Mt\n",
        "                state['exp_avg_sq'] = torch.zeros_like(grad) #Vt\n",
        "\n",
        "            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "            beta1,beta2 = group['betas']\n",
        "\n",
        "            state['step'] += 1\n",
        "\n",
        "            exp_avg.mul_(beta1).add_(grad, alpha = 1 - beta1)\n",
        "            exp_avg_sq.mul_(beta2).addcmul_(grad,grad,value = 1 - beta2)\n",
        "            denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "            step_size = group['lr']\n",
        "            if group['correct_bias']:\n",
        "              bias_correction1 = 1 - beta1 ** state['step']\n",
        "              bias_correction2 = 1 - beta2 ** state['step']\n",
        "              step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "            norm_grad = exp_avg/denom\n",
        "\n",
        "            p.add_(norm_grad,alpha = -step_size)\n",
        "\n",
        "            if group[\"weight_decay\"] > 0.0:\n",
        "                    p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "\n",
        "def print_memory_usage(self):\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    state = self.state[p]\n",
        "                    print(f\"Parameter {p.shape}:\")\n",
        "                    if 'exp_avg' in state:\n",
        "                        exp_avg_size = state['exp_avg'].numel() * state['exp_avg'].element_size()\n",
        "                        print(f\"  exp_avg: {exp_avg_size / 1024 ** 2:.2f} MB\")\n",
        "                    if 'exp_avg_sq' in state:\n",
        "                        exp_avg_sq_size = state['exp_avg_sq'].numel() * state['exp_avg_sq'].element_size()\n",
        "                        print(f\"  exp_avg_sq: {exp_avg_sq_size / 1024 ** 2:.2f} MB\")\n",
        "                    if 'projector' in state:\n",
        "                        # Assuming the projector has some tensor attributes you want to check\n",
        "                        projector_size = sum(t.numel() * t.element_size() for t in state['projector'].parameters())\n",
        "                        print(f\"  projector: {projector_size / 1024 ** 2:.2f} MB\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bGcRqmAAPgCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "import math\n",
        "import warnings\n",
        "from typing import Callable, Iterable, Tuple\n",
        "\n",
        "class AdamWGalore(Optimizer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[nn.parameter.Parameter],\n",
        "        lr: float = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-6,\n",
        "        weight_decay: float = 0.0,\n",
        "        correct_bias: bool = True,\n",
        "        no_deprecation_warning: bool = False,\n",
        "    ):\n",
        "      if lr < 0.0:\n",
        "              raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
        "      if not 0.0 <= betas[0] < 1.0:\n",
        "          raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
        "      if not 0.0 <= betas[1] < 1.0:\n",
        "          raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
        "      if not 0.0 <= eps:\n",
        "          raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
        "      defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n",
        "      super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self,closure: Callable = None):\n",
        "      \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
        "        \"\"\"\n",
        "      loss = None\n",
        "      if closure is not None:\n",
        "          loss = closure()\n",
        "\n",
        "      for group in self.param_groups:\n",
        "        for p in group['params']:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            grad = p.grad\n",
        "            if grad.is_sparse:\n",
        "              raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
        "\n",
        "            state = self.state[p]\n",
        "\n",
        "            if 'step' not in state:\n",
        "                state['step'] = 0\n",
        "\n",
        "            if 'dim'  not in group:\n",
        "              group['dim'] = 2\n",
        "\n",
        "            #Galore projection\n",
        "            if 'rank' in group:\n",
        "              if 'projector' not in group:\n",
        "                if group['dim'] <= 2:\n",
        "                  state['projector'] = GaloreProjecter(group['rank'],group['subspace_change_freq'])\n",
        "\n",
        "              grad = state['projector'].project(grad,state['step'])\n",
        "\n",
        "            #intialize the state\n",
        "            if 'exp_avg' not in state:\n",
        "                state['exp_avg'] = torch.zeros_like(grad) #Mt\n",
        "                state['exp_avg_sq'] = torch.zeros_like(grad) #Vt\n",
        "\n",
        "            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "            beta1,beta2 = group['betas']\n",
        "\n",
        "            state['step'] += 1\n",
        "\n",
        "            exp_avg.mul_(beta1).add_(grad, alpha = 1 - beta1)\n",
        "            exp_avg_sq.mul_(beta2).addcmul_(grad,grad,value = 1 - beta2)\n",
        "\n",
        "\n",
        "            step_size = group['lr']\n",
        "            if group['correct_bias']:\n",
        "              bias_correction1 = 1 - beta1 ** state['step']\n",
        "              bias_correction2 = 1 - beta2 ** state['step']\n",
        "              step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
        "              exp_avg = exp_avg/bias_correction1\n",
        "              exp_avg_sq = exp_avg_sq/bias_correction2\n",
        "\n",
        "            denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "            norm_grad = exp_avg/denom\n",
        "\n",
        "\n",
        "            if 'rank' in group:\n",
        "              norm_grad = state['projector'].projectback(norm_grad)\n",
        "\n",
        "            p.add_(norm_grad,alpha = -step_size)\n",
        "\n",
        "            if group[\"weight_decay\"] > 0.0:\n",
        "                    p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dwaONXiUaq5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rank, subspace change frequency\n",
        "def train_rank_subspace(rank,subspace_change_freq):\n",
        "  print('-----------------------------------------------------------')\n",
        "  print(f'rank: {rank}, subspace_change_freq: {subspace_change_freq}')\n",
        "  print('-----------------------------------------------------------')\n",
        "  model = GPT(GPTConfig())\n",
        "  model = torch.compile(model)\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  model.to(device)\n",
        "  torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "  train_loader = DataLoaderLite(16,1024)\n",
        "\n",
        "  torch.set_float32_matmul_precision('high')\n",
        "\n",
        "\n",
        "  target_modules_list = ['attn','mlp']\n",
        "  galore_params = []\n",
        "\n",
        "  for module_name, module in model.named_modules():\n",
        "      if not isinstance(module, nn.Linear):\n",
        "          continue\n",
        "\n",
        "      if not any(target_key in module_name for target_key in target_modules_list):\n",
        "          continue\n",
        "\n",
        "\n",
        "      galore_params.append(module.weight)\n",
        "\n",
        "  id_galore_params = [id(p) for p in galore_params]\n",
        "  # make parameters without \"rank\" to another group\n",
        "  regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
        "\n",
        "  # then call galore_adamw\n",
        "  param_groups = [{'params': regular_params},\n",
        "                  {'params': galore_params, 'rank': rank, 'subspace_change_freq': subspace_change_freq}]\n",
        "\n",
        "  ###Optimizer initialization\n",
        "  optimizer = AdamWGalore(param_groups, lr = 3e-4, betas = (0.9,0.95),eps = 1e-8)\n",
        "\n",
        "\n",
        "  epochs = 50\n",
        "  for i in range(epochs):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    x,y = train_loader.next_batch()\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.autocast(device, dtype=torch.bfloat16):\n",
        "      logits,loss = model(x,y)\n",
        "      #import code\n",
        "      #code.interact(local=locals())\n",
        "    loss.backward()\n",
        "    #Clip the gradient\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0) #sometimes you get unlucky during optimization and really high loss leads to high gradient\n",
        "    lr = get_lr(i)\n",
        "    for param_group in optimizer.param_groups:\n",
        "      param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize() #wait for gpu to finish\n",
        "    t1 = time.time()\n",
        "    dt = (t1-t0)*1000\n",
        "    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n",
        "    print(f\"step {i}, loss:{loss.item()},lr: {lr:.10f} ,norm: {norm:.4f} time {dt:.2f}ms, tokens/sec {tokens_per_sec:.2f}\")"
      ],
      "metadata": {
        "id": "_chnsv-qZshs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ranks = [1,100,500]\n",
        "subspace_change_freqs = [5,25]\n",
        "\n",
        "for rank in ranks:\n",
        "  for subspace_change_freq in subspace_change_freqs:\n",
        "    train_rank_subspace(rank,subspace_change_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kw4aOO6Mq1_C",
        "outputId": "29f567a5-6c2e-4417-9f66-c12a960815a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------\n",
            "rank: 1, subspace_change_freq: 5\n",
            "-----------------------------------------------------------\n",
            "loaded 338025 tokens\n",
            "step 0, loss:11.00564956665039,lr: 0.0000600000 ,norm: 7.1829 time 24076.71ms, tokens/sec 680.49\n",
            "step 1, loss:10.729747772216797,lr: 0.0001200000 ,norm: 7.6998 time 2989.19ms, tokens/sec 5481.08\n",
            "step 2, loss:10.444694519042969,lr: 0.0001800000 ,norm: 6.5645 time 2933.63ms, tokens/sec 5584.89\n",
            "step 3, loss:10.155477523803711,lr: 0.0002400000 ,norm: 6.4403 time 2934.95ms, tokens/sec 5582.37\n",
            "step 4, loss:9.812883377075195,lr: 0.0003000000 ,norm: 6.5733 time 2932.63ms, tokens/sec 5586.79\n",
            "step 5, loss:9.488386154174805,lr: 0.0003600000 ,norm: 5.8322 time 2957.08ms, tokens/sec 5540.60\n",
            "step 6, loss:9.120413780212402,lr: 0.0004200000 ,norm: 5.0925 time 2951.23ms, tokens/sec 5551.58\n",
            "step 7, loss:8.64113998413086,lr: 0.0004800000 ,norm: 4.0818 time 2955.20ms, tokens/sec 5544.13\n",
            "step 8, loss:8.146684646606445,lr: 0.0005400000 ,norm: 3.6008 time 2957.86ms, tokens/sec 5539.13\n",
            "step 9, loss:7.698156833648682,lr: 0.0006000000 ,norm: 3.1432 time 2942.29ms, tokens/sec 5568.46\n",
            "step 10, loss:7.31646728515625,lr: 0.0006000000 ,norm: 2.4182 time 2939.40ms, tokens/sec 5573.92\n",
            "step 11, loss:7.046461582183838,lr: 0.0005991677 ,norm: 2.1372 time 2931.25ms, tokens/sec 5589.42\n",
            "step 12, loss:6.809752464294434,lr: 0.0005966759 ,norm: 1.9268 time 2940.57ms, tokens/sec 5571.71\n",
            "step 13, loss:6.740123748779297,lr: 0.0005925399 ,norm: 1.5566 time 2948.88ms, tokens/sec 5556.01\n",
            "step 14, loss:6.721545219421387,lr: 0.0005867853 ,norm: 1.4761 time 2936.00ms, tokens/sec 5580.38\n",
            "step 15, loss:6.589057922363281,lr: 0.0005794475 ,norm: 1.7368 time 2923.68ms, tokens/sec 5603.89\n",
            "step 16, loss:6.601107120513916,lr: 0.0005705718 ,norm: 1.4379 time 2961.16ms, tokens/sec 5532.97\n",
            "step 17, loss:6.613214015960693,lr: 0.0005602128 ,norm: 1.4771 time 2938.36ms, tokens/sec 5575.89\n",
            "step 18, loss:6.666750431060791,lr: 0.0005484346 ,norm: 1.8855 time 2949.09ms, tokens/sec 5555.61\n",
            "step 19, loss:6.53225040435791,lr: 0.0005353096 ,norm: 1.8033 time 2956.28ms, tokens/sec 5542.10\n",
            "step 20, loss:6.513036251068115,lr: 0.0005209188 ,norm: 1.8716 time 2930.36ms, tokens/sec 5591.11\n",
            "step 21, loss:6.289042949676514,lr: 0.0005053510 ,norm: 1.9672 time 2928.44ms, tokens/sec 5594.78\n",
            "step 22, loss:6.374302387237549,lr: 0.0004887020 ,norm: 1.8656 time 2971.56ms, tokens/sec 5513.60\n",
            "step 23, loss:6.348262786865234,lr: 0.0004710746 ,norm: 1.7997 time 2954.04ms, tokens/sec 5546.31\n",
            "step 24, loss:6.2861008644104,lr: 0.0004525774 ,norm: 1.7213 time 2928.96ms, tokens/sec 5593.80\n",
            "step 25, loss:6.507216930389404,lr: 0.0004333245 ,norm: 1.8047 time 2942.88ms, tokens/sec 5567.34\n",
            "step 26, loss:6.566807746887207,lr: 0.0004134346 ,norm: 1.6324 time 2949.40ms, tokens/sec 5555.03\n",
            "step 27, loss:6.4173173904418945,lr: 0.0003930302 ,norm: 1.4825 time 2954.23ms, tokens/sec 5545.94\n",
            "step 28, loss:6.318214416503906,lr: 0.0003722373 ,norm: 1.5407 time 2951.18ms, tokens/sec 5551.68\n",
            "step 29, loss:6.212918281555176,lr: 0.0003511840 ,norm: 1.6898 time 2948.36ms, tokens/sec 5557.00\n",
            "step 30, loss:6.207830429077148,lr: 0.0003300000 ,norm: 1.4981 time 2946.12ms, tokens/sec 5561.22\n",
            "step 31, loss:6.2212677001953125,lr: 0.0003088160 ,norm: 1.4596 time 2953.79ms, tokens/sec 5546.78\n",
            "step 32, loss:6.138138771057129,lr: 0.0002877627 ,norm: 1.5300 time 2959.25ms, tokens/sec 5536.54\n",
            "step 33, loss:6.204935550689697,lr: 0.0002669698 ,norm: 1.4332 time 2953.12ms, tokens/sec 5548.03\n",
            "step 34, loss:6.29349946975708,lr: 0.0002465654 ,norm: 1.4343 time 2947.74ms, tokens/sec 5558.15\n",
            "step 35, loss:6.176050662994385,lr: 0.0002266755 ,norm: 1.4392 time 2969.21ms, tokens/sec 5517.98\n",
            "step 36, loss:6.15150260925293,lr: 0.0002074226 ,norm: 1.5933 time 2982.40ms, tokens/sec 5493.56\n",
            "step 37, loss:6.170697212219238,lr: 0.0001889254 ,norm: 1.6043 time 2981.56ms, tokens/sec 5495.10\n",
            "step 38, loss:6.184905529022217,lr: 0.0001712980 ,norm: 1.8736 time 2981.19ms, tokens/sec 5495.79\n",
            "step 39, loss:6.039597511291504,lr: 0.0001546490 ,norm: 1.8830 time 2982.97ms, tokens/sec 5492.51\n",
            "step 40, loss:6.116358757019043,lr: 0.0001390812 ,norm: 2.1451 time 2960.17ms, tokens/sec 5534.81\n",
            "step 41, loss:5.9025092124938965,lr: 0.0001246904 ,norm: 2.5292 time 2957.65ms, tokens/sec 5539.54\n",
            "step 42, loss:6.014497756958008,lr: 0.0001115654 ,norm: 1.8605 time 2964.34ms, tokens/sec 5527.02\n",
            "step 43, loss:5.90781307220459,lr: 0.0000997872 ,norm: 2.1012 time 3016.67ms, tokens/sec 5431.16\n",
            "step 44, loss:5.856838226318359,lr: 0.0000894282 ,norm: 1.9572 time 2982.17ms, tokens/sec 5493.98\n",
            "step 45, loss:6.053576946258545,lr: 0.0000805525 ,norm: 2.0488 time 2978.26ms, tokens/sec 5501.20\n",
            "step 46, loss:6.158290863037109,lr: 0.0000732147 ,norm: 1.9389 time 2980.93ms, tokens/sec 5496.28\n",
            "step 47, loss:6.03771448135376,lr: 0.0000674601 ,norm: 1.8130 time 3011.58ms, tokens/sec 5440.34\n",
            "step 48, loss:5.984780311584473,lr: 0.0000633241 ,norm: 1.6356 time 3003.79ms, tokens/sec 5454.45\n",
            "step 49, loss:5.903985500335693,lr: 0.0000608323 ,norm: 1.8189 time 3002.52ms, tokens/sec 5456.74\n",
            "-----------------------------------------------------------\n",
            "rank: 1, subspace_change_freq: 25\n",
            "-----------------------------------------------------------\n",
            "loaded 338025 tokens\n",
            "step 0, loss:11.00564956665039,lr: 0.0000600000 ,norm: 7.1829 time 24041.30ms, tokens/sec 681.49\n",
            "step 1, loss:10.729690551757812,lr: 0.0001200000 ,norm: 7.6998 time 2936.35ms, tokens/sec 5579.71\n",
            "step 2, loss:10.45321273803711,lr: 0.0001800000 ,norm: 6.5784 time 2935.13ms, tokens/sec 5582.03\n",
            "step 3, loss:10.159387588500977,lr: 0.0002400000 ,norm: 6.4099 time 2934.17ms, tokens/sec 5583.86\n",
            "step 4, loss:9.759298324584961,lr: 0.0003000000 ,norm: 6.3590 time 2940.55ms, tokens/sec 5571.75\n",
            "step 5, loss:9.4571533203125,lr: 0.0003600000 ,norm: 5.6899 time 2944.62ms, tokens/sec 5564.04\n",
            "step 6, loss:9.108113288879395,lr: 0.0004200000 ,norm: 5.0513 time 2977.99ms, tokens/sec 5501.70\n",
            "step 7, loss:8.648882865905762,lr: 0.0004800000 ,norm: 4.2149 time 2957.39ms, tokens/sec 5540.03\n",
            "step 8, loss:8.15812873840332,lr: 0.0005400000 ,norm: 3.8776 time 2943.80ms, tokens/sec 5565.60\n",
            "step 9, loss:7.694147109985352,lr: 0.0006000000 ,norm: 3.1692 time 2949.76ms, tokens/sec 5554.34\n",
            "step 10, loss:7.306825637817383,lr: 0.0006000000 ,norm: 2.3960 time 2951.50ms, tokens/sec 5551.07\n",
            "step 11, loss:7.036314487457275,lr: 0.0005991677 ,norm: 2.3347 time 2948.06ms, tokens/sec 5557.55\n",
            "step 12, loss:6.802275657653809,lr: 0.0005966759 ,norm: 1.9921 time 2945.78ms, tokens/sec 5561.85\n",
            "step 13, loss:6.738801002502441,lr: 0.0005925399 ,norm: 1.6886 time 2958.51ms, tokens/sec 5537.92\n",
            "step 14, loss:6.719902038574219,lr: 0.0005867853 ,norm: 1.5285 time 2969.17ms, tokens/sec 5518.03\n",
            "step 15, loss:6.587395668029785,lr: 0.0005794475 ,norm: 1.9026 time 2958.01ms, tokens/sec 5538.85\n",
            "step 16, loss:6.597195625305176,lr: 0.0005705718 ,norm: 1.5573 time 2968.15ms, tokens/sec 5519.94\n",
            "step 17, loss:6.609892845153809,lr: 0.0005602128 ,norm: 1.5477 time 2968.00ms, tokens/sec 5520.22\n",
            "step 18, loss:6.657524585723877,lr: 0.0005484346 ,norm: 1.8837 time 2967.33ms, tokens/sec 5521.47\n",
            "step 19, loss:6.520502090454102,lr: 0.0005353096 ,norm: 1.9353 time 2960.98ms, tokens/sec 5533.31\n",
            "step 20, loss:6.502026081085205,lr: 0.0005209188 ,norm: 1.9284 time 2940.67ms, tokens/sec 5571.52\n",
            "step 21, loss:6.275900840759277,lr: 0.0005053510 ,norm: 2.0618 time 2944.76ms, tokens/sec 5563.78\n",
            "step 22, loss:6.357762336730957,lr: 0.0004887020 ,norm: 1.8985 time 2934.36ms, tokens/sec 5583.50\n",
            "step 23, loss:6.330467700958252,lr: 0.0004710746 ,norm: 1.8527 time 2980.70ms, tokens/sec 5496.69\n",
            "step 24, loss:6.271393775939941,lr: 0.0004525774 ,norm: 1.7765 time 2944.37ms, tokens/sec 5564.52\n",
            "step 25, loss:6.489681243896484,lr: 0.0004333245 ,norm: 1.9552 time 2972.65ms, tokens/sec 5511.58\n",
            "step 26, loss:6.542385101318359,lr: 0.0004134346 ,norm: 1.6495 time 2985.67ms, tokens/sec 5487.54\n",
            "step 27, loss:6.394695281982422,lr: 0.0003930302 ,norm: 1.6735 time 2971.68ms, tokens/sec 5513.38\n",
            "step 28, loss:6.298761367797852,lr: 0.0003722373 ,norm: 1.6560 time 2974.71ms, tokens/sec 5507.76\n",
            "step 29, loss:6.194461822509766,lr: 0.0003511840 ,norm: 2.0789 time 2975.17ms, tokens/sec 5506.90\n",
            "step 30, loss:6.18430233001709,lr: 0.0003300000 ,norm: 1.7795 time 2968.89ms, tokens/sec 5518.55\n",
            "step 31, loss:6.202665328979492,lr: 0.0003088160 ,norm: 1.6733 time 2988.72ms, tokens/sec 5481.94\n",
            "step 32, loss:6.1162028312683105,lr: 0.0002877627 ,norm: 1.7231 time 2990.00ms, tokens/sec 5479.59\n",
            "step 33, loss:6.187504291534424,lr: 0.0002669698 ,norm: 1.6131 time 2982.84ms, tokens/sec 5492.75\n",
            "step 34, loss:6.2727580070495605,lr: 0.0002465654 ,norm: 1.5497 time 3003.18ms, tokens/sec 5455.54\n",
            "step 35, loss:6.155333995819092,lr: 0.0002266755 ,norm: 1.5303 time 3002.74ms, tokens/sec 5456.35\n",
            "step 36, loss:6.129543781280518,lr: 0.0002074226 ,norm: 1.7077 time 3007.60ms, tokens/sec 5447.53\n",
            "step 37, loss:6.143796920776367,lr: 0.0001889254 ,norm: 1.7193 time 2973.82ms, tokens/sec 5509.41\n",
            "step 38, loss:6.155136585235596,lr: 0.0001712980 ,norm: 1.9711 time 2992.36ms, tokens/sec 5475.27\n",
            "step 39, loss:6.013494491577148,lr: 0.0001546490 ,norm: 1.9905 time 3003.81ms, tokens/sec 5454.40\n",
            "step 40, loss:6.078802585601807,lr: 0.0001390812 ,norm: 2.2326 time 2984.26ms, tokens/sec 5490.14\n",
            "step 41, loss:5.86353063583374,lr: 0.0001246904 ,norm: 2.6466 time 2961.97ms, tokens/sec 5531.46\n",
            "step 42, loss:5.972525119781494,lr: 0.0001115654 ,norm: 2.0035 time 2976.35ms, tokens/sec 5504.72\n",
            "step 43, loss:5.867178916931152,lr: 0.0000997872 ,norm: 2.2573 time 3004.46ms, tokens/sec 5453.22\n",
            "step 44, loss:5.8187055587768555,lr: 0.0000894282 ,norm: 2.0941 time 2997.22ms, tokens/sec 5466.40\n",
            "step 45, loss:6.015544414520264,lr: 0.0000805525 ,norm: 2.1812 time 2984.24ms, tokens/sec 5490.18\n",
            "step 46, loss:6.11830472946167,lr: 0.0000732147 ,norm: 2.0521 time 2986.50ms, tokens/sec 5486.01\n",
            "step 47, loss:6.006704330444336,lr: 0.0000674601 ,norm: 1.9100 time 3043.65ms, tokens/sec 5383.00\n",
            "step 48, loss:5.955466270446777,lr: 0.0000633241 ,norm: 1.7161 time 3012.11ms, tokens/sec 5439.37\n",
            "step 49, loss:5.878264904022217,lr: 0.0000608323 ,norm: 1.9791 time 3007.60ms, tokens/sec 5447.54\n",
            "-----------------------------------------------------------\n",
            "rank: 100, subspace_change_freq: 5\n",
            "-----------------------------------------------------------\n",
            "loaded 338025 tokens\n",
            "step 0, loss:11.00564956665039,lr: 0.0000600000 ,norm: 7.1830 time 24128.35ms, tokens/sec 679.04\n",
            "step 1, loss:10.598737716674805,lr: 0.0001200000 ,norm: 7.5363 time 2933.40ms, tokens/sec 5585.32\n",
            "step 2, loss:10.264358520507812,lr: 0.0001800000 ,norm: 6.2290 time 2949.19ms, tokens/sec 5555.42\n",
            "step 3, loss:9.967353820800781,lr: 0.0002400000 ,norm: 5.9182 time 2945.14ms, tokens/sec 5563.06\n",
            "step 4, loss:9.515769004821777,lr: 0.0003000000 ,norm: 5.6404 time 2939.35ms, tokens/sec 5574.02\n",
            "step 5, loss:9.188411712646484,lr: 0.0003600000 ,norm: 4.8094 time 2977.20ms, tokens/sec 5503.15\n",
            "step 6, loss:8.838302612304688,lr: 0.0004200000 ,norm: 3.9395 time 2950.41ms, tokens/sec 5553.12\n",
            "step 7, loss:8.37338638305664,lr: 0.0004800000 ,norm: 3.5530 time 2951.46ms, tokens/sec 5551.15\n",
            "step 8, loss:7.935613632202148,lr: 0.0005400000 ,norm: 3.4519 time 2947.09ms, tokens/sec 5559.38\n",
            "step 9, loss:7.498471736907959,lr: 0.0006000000 ,norm: 2.6560 time 2968.66ms, tokens/sec 5518.99\n",
            "step 10, loss:7.117767333984375,lr: 0.0006000000 ,norm: 3.4266 time 2949.13ms, tokens/sec 5555.53\n",
            "step 11, loss:6.855741024017334,lr: 0.0005991677 ,norm: 2.4078 time 2943.66ms, tokens/sec 5565.86\n",
            "step 12, loss:6.605155944824219,lr: 0.0005966759 ,norm: 2.7157 time 2942.90ms, tokens/sec 5567.29\n",
            "step 13, loss:6.596376895904541,lr: 0.0005925399 ,norm: 2.1829 time 2961.12ms, tokens/sec 5533.04\n",
            "step 14, loss:6.583165168762207,lr: 0.0005867853 ,norm: 2.3105 time 2939.04ms, tokens/sec 5574.62\n",
            "step 15, loss:6.431373596191406,lr: 0.0005794475 ,norm: 3.3955 time 2915.20ms, tokens/sec 5620.20\n",
            "step 16, loss:6.412792205810547,lr: 0.0005705718 ,norm: 2.1484 time 2946.93ms, tokens/sec 5559.67\n",
            "step 17, loss:6.366369247436523,lr: 0.0005602128 ,norm: 2.2921 time 2955.72ms, tokens/sec 5543.15\n",
            "step 18, loss:6.3832197189331055,lr: 0.0005484346 ,norm: 2.5983 time 2963.93ms, tokens/sec 5527.80\n",
            "step 19, loss:6.181563377380371,lr: 0.0005353096 ,norm: 3.0666 time 2969.21ms, tokens/sec 5517.97\n",
            "step 20, loss:6.155520439147949,lr: 0.0005209188 ,norm: 4.9310 time 2933.17ms, tokens/sec 5585.77\n",
            "step 21, loss:5.868244171142578,lr: 0.0005053510 ,norm: 3.6183 time 2955.72ms, tokens/sec 5543.15\n",
            "step 22, loss:5.928418159484863,lr: 0.0004887020 ,norm: 2.6836 time 2970.05ms, tokens/sec 5516.41\n",
            "step 23, loss:5.772422790527344,lr: 0.0004710746 ,norm: 3.5309 time 2979.19ms, tokens/sec 5499.48\n",
            "step 24, loss:5.663284778594971,lr: 0.0004525774 ,norm: 2.7841 time 2981.22ms, tokens/sec 5495.73\n",
            "step 25, loss:5.837038993835449,lr: 0.0004333245 ,norm: 2.8491 time 2986.66ms, tokens/sec 5485.72\n",
            "step 26, loss:5.935059547424316,lr: 0.0004134346 ,norm: 2.1157 time 2984.47ms, tokens/sec 5489.76\n",
            "step 27, loss:5.7359113693237305,lr: 0.0003930302 ,norm: 2.7248 time 3010.46ms, tokens/sec 5442.35\n",
            "step 28, loss:5.717517375946045,lr: 0.0003722373 ,norm: 2.8657 time 2984.95ms, tokens/sec 5488.88\n",
            "step 29, loss:5.586503982543945,lr: 0.0003511840 ,norm: 2.4560 time 3013.36ms, tokens/sec 5437.12\n",
            "step 30, loss:5.588918209075928,lr: 0.0003300000 ,norm: 2.3902 time 3035.07ms, tokens/sec 5398.23\n",
            "step 31, loss:5.544602394104004,lr: 0.0003088160 ,norm: 2.2772 time 3009.69ms, tokens/sec 5443.76\n",
            "step 32, loss:5.461480140686035,lr: 0.0002877627 ,norm: 1.7534 time 3012.13ms, tokens/sec 5439.35\n",
            "step 33, loss:5.688227653503418,lr: 0.0002669698 ,norm: 2.0451 time 3014.74ms, tokens/sec 5434.62\n",
            "step 34, loss:5.831151008605957,lr: 0.0002465654 ,norm: 2.0520 time 3016.70ms, tokens/sec 5431.10\n",
            "step 35, loss:5.688166618347168,lr: 0.0002266755 ,norm: 1.9472 time 3016.17ms, tokens/sec 5432.06\n",
            "step 36, loss:5.567425727844238,lr: 0.0002074226 ,norm: 2.0336 time 3033.94ms, tokens/sec 5400.24\n",
            "step 37, loss:5.538702011108398,lr: 0.0001889254 ,norm: 1.9686 time 3023.26ms, tokens/sec 5419.31\n",
            "step 38, loss:5.600855827331543,lr: 0.0001712980 ,norm: 2.4564 time 3026.06ms, tokens/sec 5414.30\n",
            "step 39, loss:5.429173469543457,lr: 0.0001546490 ,norm: 2.0979 time 3025.28ms, tokens/sec 5415.70\n",
            "step 40, loss:5.472862720489502,lr: 0.0001390812 ,norm: 3.6306 time 2994.39ms, tokens/sec 5471.56\n",
            "step 41, loss:5.167430400848389,lr: 0.0001246904 ,norm: 3.1745 time 2995.53ms, tokens/sec 5469.48\n",
            "step 42, loss:5.264578819274902,lr: 0.0001115654 ,norm: 2.7389 time 2989.63ms, tokens/sec 5480.28\n",
            "step 43, loss:5.080621242523193,lr: 0.0000997872 ,norm: 2.3636 time 3016.85ms, tokens/sec 5430.83\n",
            "step 44, loss:5.01120662689209,lr: 0.0000894282 ,norm: 2.7692 time 3008.18ms, tokens/sec 5446.49\n",
            "step 45, loss:5.186685085296631,lr: 0.0000805525 ,norm: 2.5226 time 3013.49ms, tokens/sec 5436.89\n",
            "step 46, loss:5.384027004241943,lr: 0.0000732147 ,norm: 2.1197 time 3012.20ms, tokens/sec 5439.21\n",
            "step 47, loss:5.232962131500244,lr: 0.0000674601 ,norm: 2.0179 time 3030.47ms, tokens/sec 5406.42\n",
            "step 48, loss:5.304534435272217,lr: 0.0000633241 ,norm: 2.0117 time 3019.47ms, tokens/sec 5426.11\n",
            "step 49, loss:5.193730354309082,lr: 0.0000608323 ,norm: 2.8163 time 3028.32ms, tokens/sec 5410.27\n",
            "-----------------------------------------------------------\n",
            "rank: 100, subspace_change_freq: 25\n",
            "-----------------------------------------------------------\n",
            "loaded 338025 tokens\n",
            "step 0, loss:11.00564956665039,lr: 0.0000600000 ,norm: 7.1830 time 24194.19ms, tokens/sec 677.19\n",
            "step 1, loss:10.598733901977539,lr: 0.0001200000 ,norm: 7.5342 time 2933.80ms, tokens/sec 5584.57\n",
            "step 2, loss:10.30758285522461,lr: 0.0001800000 ,norm: 6.3511 time 2928.03ms, tokens/sec 5595.58\n",
            "step 3, loss:10.013561248779297,lr: 0.0002400000 ,norm: 6.0383 time 2941.37ms, tokens/sec 5570.19\n",
            "step 4, loss:9.566644668579102,lr: 0.0003000000 ,norm: 5.7717 time 2934.47ms, tokens/sec 5583.30\n",
            "step 5, loss:9.242186546325684,lr: 0.0003600000 ,norm: 5.0463 time 2951.95ms, tokens/sec 5550.23\n",
            "step 6, loss:8.884363174438477,lr: 0.0004200000 ,norm: 4.2177 time 2940.68ms, tokens/sec 5571.51\n",
            "step 7, loss:8.402960777282715,lr: 0.0004800000 ,norm: 3.6721 time 2953.82ms, tokens/sec 5546.71\n",
            "step 8, loss:7.957792282104492,lr: 0.0005400000 ,norm: 3.3610 time 2957.06ms, tokens/sec 5540.63\n",
            "step 9, loss:7.501176357269287,lr: 0.0006000000 ,norm: 2.6239 time 2956.69ms, tokens/sec 5541.33\n",
            "step 10, loss:7.141526222229004,lr: 0.0006000000 ,norm: 2.6567 time 2942.31ms, tokens/sec 5568.40\n",
            "step 11, loss:6.867559909820557,lr: 0.0005991677 ,norm: 3.1351 time 2933.80ms, tokens/sec 5584.57\n",
            "step 12, loss:6.618862152099609,lr: 0.0005966759 ,norm: 3.1118 time 2947.99ms, tokens/sec 5557.69\n",
            "step 13, loss:6.608660697937012,lr: 0.0005925399 ,norm: 2.1395 time 2996.94ms, tokens/sec 5466.90\n",
            "step 14, loss:6.583364009857178,lr: 0.0005867853 ,norm: 1.8388 time 2968.18ms, tokens/sec 5519.87\n",
            "step 15, loss:6.419214248657227,lr: 0.0005794475 ,norm: 3.2179 time 2948.02ms, tokens/sec 5557.62\n",
            "step 16, loss:6.3956499099731445,lr: 0.0005705718 ,norm: 2.0709 time 2978.98ms, tokens/sec 5499.88\n",
            "step 17, loss:6.335261344909668,lr: 0.0005602128 ,norm: 2.6456 time 2962.75ms, tokens/sec 5530.00\n",
            "step 18, loss:6.349687576293945,lr: 0.0005484346 ,norm: 2.6235 time 2990.58ms, tokens/sec 5478.53\n",
            "step 19, loss:6.134573936462402,lr: 0.0005353096 ,norm: 2.5602 time 2992.78ms, tokens/sec 5474.51\n",
            "step 20, loss:6.099766731262207,lr: 0.0005209188 ,norm: 3.6296 time 2984.61ms, tokens/sec 5489.49\n",
            "step 21, loss:5.802401542663574,lr: 0.0005053510 ,norm: 3.0715 time 2973.00ms, tokens/sec 5510.93\n",
            "step 22, loss:5.864299774169922,lr: 0.0004887020 ,norm: 2.8265 time 2983.75ms, tokens/sec 5491.07\n",
            "step 23, loss:5.735997200012207,lr: 0.0004710746 ,norm: 3.5338 time 2991.15ms, tokens/sec 5477.50\n",
            "step 24, loss:5.6286749839782715,lr: 0.0004525774 ,norm: 3.4575 time 2987.02ms, tokens/sec 5485.06\n",
            "step 25, loss:5.812071800231934,lr: 0.0004333245 ,norm: 3.3705 time 2990.90ms, tokens/sec 5477.95\n",
            "step 26, loss:5.9179277420043945,lr: 0.0004134346 ,norm: 2.8448 time 2978.59ms, tokens/sec 5500.59\n",
            "step 27, loss:5.720329284667969,lr: 0.0003930302 ,norm: 3.0927 time 3037.59ms, tokens/sec 5393.76\n",
            "step 28, loss:5.696698188781738,lr: 0.0003722373 ,norm: 3.2537 time 3001.76ms, tokens/sec 5458.13\n",
            "step 29, loss:5.562747001647949,lr: 0.0003511840 ,norm: 2.8860 time 3035.67ms, tokens/sec 5397.16\n",
            "step 30, loss:5.552814960479736,lr: 0.0003300000 ,norm: 1.6831 time 3030.96ms, tokens/sec 5405.55\n",
            "step 31, loss:5.559239387512207,lr: 0.0003088160 ,norm: 1.7177 time 3020.43ms, tokens/sec 5424.40\n",
            "step 32, loss:5.46060848236084,lr: 0.0002877627 ,norm: 1.9044 time 3025.91ms, tokens/sec 5414.57\n",
            "step 33, loss:5.6763176918029785,lr: 0.0002669698 ,norm: 3.6998 time 3001.70ms, tokens/sec 5458.24\n",
            "step 34, loss:5.799957275390625,lr: 0.0002465654 ,norm: 3.0283 time 3054.98ms, tokens/sec 5363.05\n",
            "step 35, loss:5.677149295806885,lr: 0.0002266755 ,norm: 1.9122 time 3042.33ms, tokens/sec 5385.34\n",
            "step 36, loss:5.552432060241699,lr: 0.0002074226 ,norm: 2.2855 time 3048.27ms, tokens/sec 5374.86\n",
            "step 37, loss:5.532386302947998,lr: 0.0001889254 ,norm: 2.1122 time 3025.37ms, tokens/sec 5415.53\n",
            "step 38, loss:5.576565742492676,lr: 0.0001712980 ,norm: 2.5905 time 3030.95ms, tokens/sec 5405.57\n",
            "step 39, loss:5.390587329864502,lr: 0.0001546490 ,norm: 2.2931 time 3040.13ms, tokens/sec 5389.24\n",
            "step 40, loss:5.419117450714111,lr: 0.0001390812 ,norm: 4.3100 time 3005.47ms, tokens/sec 5451.39\n",
            "step 41, loss:5.121147632598877,lr: 0.0001246904 ,norm: 3.9526 time 3013.35ms, tokens/sec 5437.14\n",
            "step 42, loss:5.240777015686035,lr: 0.0001115654 ,norm: 2.8375 time 3023.38ms, tokens/sec 5419.10\n",
            "step 43, loss:5.062324523925781,lr: 0.0000997872 ,norm: 2.6346 time 3018.18ms, tokens/sec 5428.44\n",
            "step 44, loss:5.01334285736084,lr: 0.0000894282 ,norm: 3.1639 time 3026.03ms, tokens/sec 5414.36\n",
            "step 45, loss:5.192421913146973,lr: 0.0000805525 ,norm: 2.7154 time 3023.85ms, tokens/sec 5418.26\n",
            "step 46, loss:5.396137237548828,lr: 0.0000732147 ,norm: 2.2308 time 3018.30ms, tokens/sec 5428.21\n",
            "step 47, loss:5.230917930603027,lr: 0.0000674601 ,norm: 2.1926 time 3032.59ms, tokens/sec 5402.65\n",
            "step 48, loss:5.288280487060547,lr: 0.0000633241 ,norm: 2.4135 time 3013.11ms, tokens/sec 5437.57\n",
            "step 49, loss:5.179717540740967,lr: 0.0000608323 ,norm: 3.2012 time 3015.05ms, tokens/sec 5434.07\n",
            "-----------------------------------------------------------\n",
            "rank: 500, subspace_change_freq: 5\n",
            "-----------------------------------------------------------\n",
            "loaded 338025 tokens\n",
            "step 0, loss:11.00564956665039,lr: 0.0000600000 ,norm: 7.1829 time 23857.15ms, tokens/sec 686.75\n",
            "step 1, loss:10.57288932800293,lr: 0.0001200000 ,norm: 7.4981 time 2940.25ms, tokens/sec 5572.31\n",
            "step 2, loss:10.229080200195312,lr: 0.0001800000 ,norm: 6.1790 time 2931.58ms, tokens/sec 5588.79\n",
            "step 3, loss:9.948783874511719,lr: 0.0002400000 ,norm: 5.9005 time 2941.03ms, tokens/sec 5570.84\n",
            "step 4, loss:9.487335205078125,lr: 0.0003000000 ,norm: 5.6101 time 2941.85ms, tokens/sec 5569.29\n",
            "step 5, loss:9.154989242553711,lr: 0.0003600000 ,norm: 4.7710 time 2948.43ms, tokens/sec 5556.85\n",
            "step 6, loss:8.829407691955566,lr: 0.0004200000 ,norm: 4.0829 time 2933.64ms, tokens/sec 5584.88\n",
            "step 7, loss:8.353452682495117,lr: 0.0004800000 ,norm: 3.5723 time 2954.01ms, tokens/sec 5546.37\n",
            "step 8, loss:7.915821075439453,lr: 0.0005400000 ,norm: 3.2332 time 2946.79ms, tokens/sec 5559.94\n",
            "step 9, loss:7.483434677124023,lr: 0.0006000000 ,norm: 2.9662 time 2960.32ms, tokens/sec 5534.54\n",
            "step 10, loss:7.103762626647949,lr: 0.0006000000 ,norm: 3.6454 time 2930.09ms, tokens/sec 5591.63\n",
            "step 11, loss:6.830781936645508,lr: 0.0005991677 ,norm: 3.6229 time 2926.47ms, tokens/sec 5598.56\n",
            "step 12, loss:6.5846405029296875,lr: 0.0005966759 ,norm: 2.9797 time 2942.45ms, tokens/sec 5568.15\n",
            "step 13, loss:6.573351860046387,lr: 0.0005925399 ,norm: 2.0207 time 2950.99ms, tokens/sec 5552.04\n",
            "step 14, loss:6.555927753448486,lr: 0.0005867853 ,norm: 1.8274 time 2954.98ms, tokens/sec 5544.53\n",
            "step 15, loss:6.382049083709717,lr: 0.0005794475 ,norm: 2.0590 time 2947.94ms, tokens/sec 5557.79\n",
            "step 16, loss:6.3436384201049805,lr: 0.0005705718 ,norm: 2.3942 time 2974.84ms, tokens/sec 5507.53\n",
            "step 17, loss:6.283150672912598,lr: 0.0005602128 ,norm: 2.5308 time 3000.14ms, tokens/sec 5461.08\n",
            "step 18, loss:6.299508094787598,lr: 0.0005484346 ,norm: 3.2753 time 2963.30ms, tokens/sec 5528.97\n",
            "step 19, loss:6.080293655395508,lr: 0.0005353096 ,norm: 3.1417 time 2981.51ms, tokens/sec 5495.19\n",
            "step 20, loss:6.006708145141602,lr: 0.0005209188 ,norm: 4.8520 time 2942.78ms, tokens/sec 5567.52\n",
            "step 21, loss:5.703358173370361,lr: 0.0005053510 ,norm: 3.8151 time 2976.30ms, tokens/sec 5504.82\n",
            "step 22, loss:5.752519607543945,lr: 0.0004887020 ,norm: 3.6995 time 2960.02ms, tokens/sec 5535.09\n",
            "step 23, loss:5.612866401672363,lr: 0.0004710746 ,norm: 3.8138 time 3008.40ms, tokens/sec 5446.09\n",
            "step 24, loss:5.488409042358398,lr: 0.0004525774 ,norm: 3.2197 time 3007.79ms, tokens/sec 5447.19\n",
            "step 25, loss:5.659027099609375,lr: 0.0004333245 ,norm: 3.6012 time 3006.84ms, tokens/sec 5448.90\n",
            "step 26, loss:5.7690815925598145,lr: 0.0004134346 ,norm: 2.4272 time 3014.74ms, tokens/sec 5434.64\n",
            "step 27, loss:5.548240661621094,lr: 0.0003930302 ,norm: 2.7035 time 3035.29ms, tokens/sec 5397.83\n",
            "step 28, loss:5.5628156661987305,lr: 0.0003722373 ,norm: 3.2418 time 3008.46ms, tokens/sec 5445.98\n",
            "step 29, loss:5.43327522277832,lr: 0.0003511840 ,norm: 2.3727 time 3032.26ms, tokens/sec 5403.23\n",
            "step 30, loss:5.447302341461182,lr: 0.0003300000 ,norm: 2.0640 time 3022.21ms, tokens/sec 5421.20\n",
            "step 31, loss:5.4428510665893555,lr: 0.0003088160 ,norm: 2.3239 time 3012.64ms, tokens/sec 5438.42\n",
            "step 32, loss:5.326203346252441,lr: 0.0002877627 ,norm: 2.4870 time 3009.44ms, tokens/sec 5444.20\n",
            "step 33, loss:5.540117263793945,lr: 0.0002669698 ,norm: 3.1808 time 3010.48ms, tokens/sec 5442.33\n",
            "step 34, loss:5.681585311889648,lr: 0.0002465654 ,norm: 2.4682 time 3025.45ms, tokens/sec 5415.39\n",
            "step 35, loss:5.509124755859375,lr: 0.0002266755 ,norm: 1.8683 time 3005.42ms, tokens/sec 5451.49\n",
            "step 36, loss:5.431148529052734,lr: 0.0002074226 ,norm: 2.4694 time 3015.06ms, tokens/sec 5434.05\n",
            "step 37, loss:5.391313552856445,lr: 0.0001889254 ,norm: 2.3090 time 3031.22ms, tokens/sec 5405.09\n",
            "step 38, loss:5.491527557373047,lr: 0.0001712980 ,norm: 2.7373 time 3008.72ms, tokens/sec 5445.50\n",
            "step 39, loss:5.295413017272949,lr: 0.0001546490 ,norm: 2.4546 time 3016.31ms, tokens/sec 5431.80\n",
            "step 40, loss:5.2866597175598145,lr: 0.0001390812 ,norm: 4.4523 time 2986.73ms, tokens/sec 5485.60\n",
            "step 41, loss:4.984593868255615,lr: 0.0001246904 ,norm: 4.1131 time 3004.73ms, tokens/sec 5452.73\n",
            "step 42, loss:5.090747833251953,lr: 0.0001115654 ,norm: 3.3069 time 2985.78ms, tokens/sec 5487.35\n",
            "step 43, loss:4.868269920349121,lr: 0.0000997872 ,norm: 2.8780 time 2998.33ms, tokens/sec 5464.38\n",
            "step 44, loss:4.78617000579834,lr: 0.0000894282 ,norm: 3.2099 time 2988.06ms, tokens/sec 5483.16\n",
            "step 45, loss:4.97654914855957,lr: 0.0000805525 ,norm: 2.7156 time 2990.37ms, tokens/sec 5478.93\n",
            "step 46, loss:5.148644924163818,lr: 0.0000732147 ,norm: 2.4253 time 3012.93ms, tokens/sec 5437.89\n",
            "step 47, loss:4.986598014831543,lr: 0.0000674601 ,norm: 2.2034 time 3019.77ms, tokens/sec 5425.58\n",
            "step 48, loss:5.107746124267578,lr: 0.0000633241 ,norm: 2.5294 time 2995.56ms, tokens/sec 5469.42\n",
            "step 49, loss:4.986064434051514,lr: 0.0000608323 ,norm: 3.4377 time 3007.03ms, tokens/sec 5448.56\n",
            "-----------------------------------------------------------\n",
            "rank: 500, subspace_change_freq: 25\n",
            "-----------------------------------------------------------\n",
            "loaded 338025 tokens\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2d8779619ef1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msubspace_change_freq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubspace_change_freqs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_rank_subspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubspace_change_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-38975a99862e>\u001b[0m in \u001b[0;36mtrain_rank_subspace\u001b[0;34m(rank, subspace_change_freq)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;31m#import code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;31m#code.interact(local=locals())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcatch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0mcatch_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchdynamo_orig_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             result = inner_convert(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         return _compile(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mfail_user_frame_lineno\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mguarded_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         except (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mCompileContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestartAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mcleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_compile_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcleanup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                 \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnspecializeRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mspeculation_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch_nested_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_pointer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                 ):\n\u001b[1;32m    812\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             )\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"RETURN_VALUE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mRETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         )\n\u001b[1;32m   2267\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RETURN_VALUE triggered compile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m         self.output.compile_subgraph(\n\u001b[0m\u001b[1;32m   2269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m             reason=GraphCompileReason(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                 output.extend(\n\u001b[0;32m--> 991\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_call_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_output_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m                 )\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_global_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_correctness\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mcompiler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWrapperBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m             \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"done compiler function {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compiler_fn did not return callable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_gm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_fx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_fx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_compiler_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0mtracing_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     ), compiled_autograd.disable():\n\u001b[0;32m-> 1330\u001b[0;31m         return aot_autograd(\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mfw_compiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_compiler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0mbw_compiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbw_compiler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\u001b[0m in \u001b[0;36mcompiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# NB: NOT cloned!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0menable_aot_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maot_module_simplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aot_autograd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcompiled_autograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         compiled_fn = create_aot_dispatcher_function(\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mfunctional_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;31m# You can put more passes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_flat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfw_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_export\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;31m# During export, we don't get back a callable - we get back the raw fx graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_flat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfw_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequires_subclass_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_flat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfw_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36maot_wrapper_synthetic_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# Happy path: we don't need synthetic bases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msynthetic_base_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfw_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;31m# export path: ban synthetic bases for now, add later if requested.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\u001b[0m in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_output_strides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfwd_output_strides\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mcompiled_fw_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfw_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fw_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mcompiled_fw_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_boxed_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fw_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mfw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1255\u001b[0m             }\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         return inner_compile(\n\u001b[0m\u001b[1;32m   1258\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# with fake inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0minner_compiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# TODO: Failures here are troublesome because no real inputs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mDebugContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_compiler_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiler_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inductor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    436\u001b[0m         )\n\u001b[1;32m    437\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         compiled_graph = fx_codegen_and_compile(\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgraph_kwargs\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mmetrics_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCachedMetricsHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maot_compilation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m             )\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         code, linemap = (\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodegen_with_cpp_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_wrapper\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         )\n\u001b[1;32m   1252\u001b[0m         \u001b[0mlinemap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_trace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinemap\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcodegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_wrapper_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_orig_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_gm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopological_sort_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogged_slow_fusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_for_compute_comm_overlap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;31m# Refresh node_users and inverse_users to reflect fused nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mfuse_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                 \u001b[0;34m\"===== attempting fusion (%d/10): %d nodes =====\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m             )\n\u001b[0;32m-> 1686\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_nodes_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m             \u001b[0mnew_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m             fusion_log.debug(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mfuse_nodes_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \"\"\"\n\u001b[1;32m   1815\u001b[0m         \u001b[0mfused_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnode1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_possible_fusions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1817\u001b[0m             \u001b[0mnode1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_to_fused_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_first_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m             \u001b[0mnode2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_to_fused_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_first_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mget_possible_fusions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1871\u001b[0m                 \u001b[0mbuffer_names_grouping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_grouping\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbuffer_names_grouping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m             \u001b[0mcheck_all_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_grouping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggressive_fusion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mcheck_all_pairs\u001b[0;34m(nodes)\u001b[0m\n\u001b[1;32m   1858\u001b[0m                     \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_fuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m                         \u001b[0mpossible_fusions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                     elif (node2.is_template() or node2.is_foreach()) and self.can_fuse(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mcan_fuse\u001b[0;34m(self, node1, node2)\u001b[0m\n\u001b[1;32m   2015\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mdevice2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2017\u001b[0;31m         \u001b[0mno_shared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_fusion_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2018\u001b[0m         if no_shared_data and (\n\u001b[1;32m   2019\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggressive_fusion\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnode1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnode2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mscore_fusion_memory\u001b[0;34m(self, node1, node2)\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0mnode2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_writes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreads\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnode2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_writes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m         )\n\u001b[0;32m-> 2134\u001b[0;31m         common_memory_deps = {\n\u001b[0m\u001b[1;32m   2135\u001b[0m             \u001b[0mdep\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_memory_deps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_unbacked_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m         }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0mnode2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_writes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreads\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnode2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_writes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m         )\n\u001b[0;32m-> 2134\u001b[0;31m         common_memory_deps = {\n\u001b[0m\u001b[1;32m   2135\u001b[0m             \u001b[0mdep\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_memory_deps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_unbacked_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m         }\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal AdamW implementation"
      ],
      "metadata": {
        "id": "_sgEKnrxqRXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig())\n",
        "model = torch.compile(model)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "train_loader = DataLoaderLite(16,1024)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "###Optimizer initialization\n",
        "optimizer = AdamW(param_groups, lr = 3e-4, betas = (0.9,0.95),eps = 1e-8)\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "for i in range(epochs):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "  x,y = train_loader.next_batch()\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  with torch.autocast(device, dtype=torch.bfloat16):\n",
        "    logits,loss = model(x,y)\n",
        "    #import code\n",
        "    #code.interact(local=locals())\n",
        "  loss.backward()\n",
        "  #Clip the gradient\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0) #sometimes you get unlucky during optimization and really high loss leads to high gradient\n",
        "  lr = get_lr(i)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize() #wait for gpu to finish\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n",
        "  print(f\"step {i}, loss:{loss.item()},lr: {lr:.10f} ,norm: {norm:.4f} time {dt:.2f}ms, tokens/sec {tokens_per_sec:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvDspyWZZO9Z",
        "outputId": "40892cc6-1aee-403d-fd6d-73960afb1d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens\n",
            "step 0, loss:11.00564956665039,lr: 0.0000600000 ,norm: 7.1830 time 41452.72ms, tokens/sec 395.25\n",
            "step 1, loss:11.014896392822266,lr: 0.0001200000 ,norm: 8.9117 time 102.46ms, tokens/sec 159912.40\n",
            "step 2, loss:11.011943817138672,lr: 0.0001800000 ,norm: 7.9937 time 101.94ms, tokens/sec 160716.49\n",
            "step 3, loss:10.99173355102539,lr: 0.0002400000 ,norm: 7.9452 time 102.49ms, tokens/sec 159852.51\n",
            "step 4, loss:11.011993408203125,lr: 0.0003000000 ,norm: 8.1919 time 102.21ms, tokens/sec 160291.37\n",
            "step 5, loss:11.006282806396484,lr: 0.0003600000 ,norm: 7.5789 time 102.31ms, tokens/sec 160134.87\n",
            "step 6, loss:10.990325927734375,lr: 0.0004200000 ,norm: 7.1289 time 101.96ms, tokens/sec 160690.93\n",
            "step 7, loss:10.999454498291016,lr: 0.0004800000 ,norm: 7.2375 time 102.28ms, tokens/sec 160181.90\n",
            "step 8, loss:10.989326477050781,lr: 0.0005400000 ,norm: 7.9409 time 102.77ms, tokens/sec 159429.73\n",
            "step 9, loss:10.975311279296875,lr: 0.0006000000 ,norm: 7.8172 time 102.59ms, tokens/sec 159708.00\n",
            "step 10, loss:10.978080749511719,lr: 0.0006000000 ,norm: 7.8849 time 102.77ms, tokens/sec 159418.64\n",
            "step 11, loss:10.974468231201172,lr: 0.0005991677 ,norm: 7.6083 time 102.68ms, tokens/sec 159568.19\n",
            "step 12, loss:10.98917007446289,lr: 0.0005966759 ,norm: 7.9038 time 101.82ms, tokens/sec 160904.27\n",
            "step 13, loss:10.999439239501953,lr: 0.0005925399 ,norm: 7.9315 time 102.13ms, tokens/sec 160429.08\n",
            "step 14, loss:11.003204345703125,lr: 0.0005867853 ,norm: 7.7092 time 101.87ms, tokens/sec 160837.61\n",
            "step 15, loss:11.005126953125,lr: 0.0005794475 ,norm: 8.0847 time 101.96ms, tokens/sec 160683.04\n",
            "step 16, loss:11.017066955566406,lr: 0.0005705718 ,norm: 8.1119 time 102.06ms, tokens/sec 160534.77\n",
            "step 17, loss:11.01175308227539,lr: 0.0005602128 ,norm: 7.8744 time 101.67ms, tokens/sec 161145.37\n",
            "step 18, loss:10.994720458984375,lr: 0.0005484346 ,norm: 7.9985 time 102.01ms, tokens/sec 160608.31\n",
            "step 19, loss:10.991962432861328,lr: 0.0005353096 ,norm: 8.5888 time 102.24ms, tokens/sec 160247.27\n",
            "step 20, loss:11.00564956665039,lr: 0.0005209188 ,norm: 8.1263 time 102.61ms, tokens/sec 159668.66\n",
            "step 21, loss:11.014896392822266,lr: 0.0005053510 ,norm: 8.9118 time 103.42ms, tokens/sec 158421.21\n",
            "step 22, loss:11.011943817138672,lr: 0.0004887020 ,norm: 7.9939 time 103.14ms, tokens/sec 158857.38\n",
            "step 23, loss:10.99173355102539,lr: 0.0004710746 ,norm: 7.9453 time 102.61ms, tokens/sec 159667.55\n",
            "step 24, loss:11.011993408203125,lr: 0.0004525774 ,norm: 8.1917 time 102.44ms, tokens/sec 159932.50\n",
            "step 25, loss:11.006282806396484,lr: 0.0004333245 ,norm: 7.5788 time 102.55ms, tokens/sec 159771.49\n",
            "step 26, loss:10.990325927734375,lr: 0.0004134346 ,norm: 7.1289 time 102.12ms, tokens/sec 160435.82\n",
            "step 27, loss:10.999454498291016,lr: 0.0003930302 ,norm: 7.2375 time 102.15ms, tokens/sec 160383.78\n",
            "step 28, loss:10.989326477050781,lr: 0.0003722373 ,norm: 7.9410 time 102.22ms, tokens/sec 160278.66\n",
            "step 29, loss:10.975311279296875,lr: 0.0003511840 ,norm: 7.8172 time 102.15ms, tokens/sec 160394.26\n",
            "step 30, loss:10.978080749511719,lr: 0.0003300000 ,norm: 7.8848 time 102.03ms, tokens/sec 160578.66\n",
            "step 31, loss:10.974468231201172,lr: 0.0003088160 ,norm: 7.6085 time 102.80ms, tokens/sec 159380.19\n",
            "step 32, loss:10.98917007446289,lr: 0.0002877627 ,norm: 7.9039 time 103.09ms, tokens/sec 158931.22\n",
            "step 33, loss:10.999439239501953,lr: 0.0002669698 ,norm: 7.9315 time 103.08ms, tokens/sec 158944.46\n",
            "step 34, loss:11.003204345703125,lr: 0.0002465654 ,norm: 7.7091 time 102.18ms, tokens/sec 160350.10\n",
            "step 35, loss:11.005126953125,lr: 0.0002266755 ,norm: 8.0847 time 102.21ms, tokens/sec 160291.00\n",
            "step 36, loss:11.017066955566406,lr: 0.0002074226 ,norm: 8.1119 time 101.80ms, tokens/sec 160946.47\n",
            "step 37, loss:11.01175308227539,lr: 0.0001889254 ,norm: 7.8743 time 102.24ms, tokens/sec 160254.74\n",
            "step 38, loss:10.994720458984375,lr: 0.0001712980 ,norm: 7.9985 time 102.25ms, tokens/sec 160239.79\n",
            "step 39, loss:10.991962432861328,lr: 0.0001546490 ,norm: 8.5889 time 102.33ms, tokens/sec 160114.35\n",
            "step 40, loss:11.00564956665039,lr: 0.0001390812 ,norm: 8.1262 time 101.97ms, tokens/sec 160678.16\n",
            "step 41, loss:11.014896392822266,lr: 0.0001246904 ,norm: 8.9117 time 103.27ms, tokens/sec 158649.06\n",
            "step 42, loss:11.011943817138672,lr: 0.0001115654 ,norm: 7.9939 time 103.28ms, tokens/sec 158640.27\n",
            "step 43, loss:10.99173355102539,lr: 0.0000997872 ,norm: 7.9453 time 103.17ms, tokens/sec 158803.04\n",
            "step 44, loss:11.011993408203125,lr: 0.0000894282 ,norm: 8.1918 time 102.62ms, tokens/sec 159660.87\n",
            "step 45, loss:11.006282806396484,lr: 0.0000805525 ,norm: 7.5789 time 102.74ms, tokens/sec 159474.13\n",
            "step 46, loss:10.990325927734375,lr: 0.0000732147 ,norm: 7.1288 time 101.93ms, tokens/sec 160740.92\n",
            "step 47, loss:10.999454498291016,lr: 0.0000674601 ,norm: 7.2374 time 101.80ms, tokens/sec 160935.54\n",
            "step 48, loss:10.989326477050781,lr: 0.0000633241 ,norm: 7.9410 time 102.48ms, tokens/sec 159867.76\n",
            "step 49, loss:10.975311279296875,lr: 0.0000608323 ,norm: 7.8172 time 102.31ms, tokens/sec 160134.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzS-wAm-b3bL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}